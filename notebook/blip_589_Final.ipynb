{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "START"
      ],
      "metadata": {
        "id": "-6CRnap9i1zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "kx26bIWvnIrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "candidates = glob.glob(\"/content/drive/MyDrive/**/*.zip\", recursive=True)\n",
        "# Show only zip files that look like flickr8k\n",
        "[f for f in candidates if \"flickr\" in f.lower() or \"flickr8k\" in f.lower()][:20]\n"
      ],
      "metadata": {
        "id": "Fi-rcgo7n9y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ZIP_PATH = \"/content/drive/MyDrive/flickr8k.zip\"\n",
        "OUT_DIR = \"/content/data/flickr8k\"\n",
        "\n",
        "!mkdir -p \"$OUT_DIR\"\n",
        "!unzip -q \"$ZIP_PATH\" -d \"$OUT_DIR\"\n",
        "print(\"Unzipped to:\", OUT_DIR)\n"
      ],
      "metadata": {
        "id": "p88Ji1nGonf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "OUT_DIR = \"/content/data/flickr8k\"\n",
        "print(\"Top-level folders/files inside OUT_DIR:\")\n",
        "print(os.listdir(OUT_DIR)[:50])\n"
      ],
      "metadata": {
        "id": "uKXfLEFPo_Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "OUT_DIR = \"/content/data/flickr8k\"\n",
        "images_dir = os.path.join(OUT_DIR, \"Images\")\n",
        "captions_file = os.path.join(OUT_DIR, \"captions.txt\")\n",
        "\n",
        "print(\"Images dir exists:\", os.path.isdir(images_dir))\n",
        "print(\"Num images:\", len([f for f in os.listdir(images_dir) if f.lower().endswith(\".jpg\")]))\n",
        "\n",
        "print(\"Captions file exists:\", os.path.isfile(captions_file))\n",
        "with open(captions_file, \"r\") as f:\n",
        "    for _ in range(5):\n",
        "        print(f.readline().strip())\n"
      ],
      "metadata": {
        "id": "6BXiFNeXoy5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOST IMORTANT CELL FOR EXECUTION"
      ],
      "metadata": {
        "id": "mLjtT8X7pYpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        ").to(device)\n",
        "\n",
        "print(\"Model loaded on\", device)\n"
      ],
      "metadata": {
        "id": "WVRi7cy8pbaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REBUILDING"
      ],
      "metadata": {
        "id": "iBSwLDVfrApG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, tokenizers\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"tokenizers:\", tokenizers.__version__)\n"
      ],
      "metadata": {
        "id": "28L8HOaeuWB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "print(\"Model loaded\")\n"
      ],
      "metadata": {
        "id": "oXXlaA6fuggh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_DIR = \"/content/data/flickr8k\"\n",
        "images_dir = os.path.join(OUT_DIR, \"Images\")\n",
        "captions_file = os.path.join(OUT_DIR, \"captions.txt\")\n",
        "\n",
        "print(\"Images:\", len([f for f in os.listdir(images_dir) if f.lower().endswith(\".jpg\")]))\n",
        "print(\"Captions file exists:\", os.path.isfile(captions_file))\n"
      ],
      "metadata": {
        "id": "IznFKNnpukoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "rows = []\n",
        "with open(captions_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if \",\" in line:\n",
        "            img, cap = line.split(\",\", 1)\n",
        "        else:\n",
        "            continue\n",
        "        img, cap = img.strip(), cap.strip()\n",
        "        if img.lower() == \"image\" and cap.lower() == \"caption\":\n",
        "            continue\n",
        "        rows.append({\"image\": img, \"caption\": cap})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# split by image (80/10/10)\n",
        "unique_images = df[\"image\"].unique()\n",
        "rng = np.random.default_rng(42)\n",
        "rng.shuffle(unique_images)\n",
        "\n",
        "n = len(unique_images)\n",
        "train_end = int(0.8 * n)\n",
        "val_end   = int(0.9 * n)\n",
        "\n",
        "train_imgs = set(unique_images[:train_end])\n",
        "val_imgs   = set(unique_images[train_end:val_end])\n",
        "test_imgs  = set(unique_images[val_end:])\n",
        "\n",
        "def assign_split(img):\n",
        "    if img in train_imgs: return \"train\"\n",
        "    if img in val_imgs: return \"val\"\n",
        "    return \"test\"\n",
        "\n",
        "df[\"split\"] = df[\"image\"].apply(assign_split)\n",
        "\n",
        "print(\"Rows per split:\")\n",
        "print(df[\"split\"].value_counts())\n",
        "print(\"\\nUnique images per split:\")\n",
        "print(df.groupby(\"split\")[\"image\"].nunique())\n"
      ],
      "metadata": {
        "id": "sxP-ip81vATY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "MAX_TEXT_LEN = 32\n",
        "\n",
        "class FlickrCaptionDataset(Dataset):\n",
        "    def __init__(self, df, split):\n",
        "        d = df[df[\"split\"] == split].reset_index(drop=True)\n",
        "        self.grouped = d.groupby(\"image\")[\"caption\"].apply(list).reset_index()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.grouped)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.grouped.iloc[idx]\n",
        "        img_name = row[\"image\"]\n",
        "        caption = random.choice(row[\"caption\"])\n",
        "\n",
        "        img_path = os.path.join(images_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        enc = processor(\n",
        "            images=image,\n",
        "            text=caption,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_TEXT_LEN,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "\n",
        "        labels = item[\"input_ids\"].clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "        item[\"labels\"] = labels\n",
        "        return item\n",
        "\n",
        "def collate_fn(batch):\n",
        "    keys = batch[0].keys()\n",
        "    return {k: torch.stack([b[k] for b in batch]) for k in keys}\n",
        "\n",
        "train_dataset = FlickrCaptionDataset(df, \"train\")\n",
        "val_dataset   = FlickrCaptionDataset(df, \"val\")\n",
        "test_dataset  = FlickrCaptionDataset(df, \"test\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n"
      ],
      "metadata": {
        "id": "JO8YdpAYvEKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total += loss.item()\n",
        "        n += 1\n",
        "\n",
        "        if train and step % 100 == 0:\n",
        "            print(f\"step {step}/{len(loader)}  loss={loss.item():.4f}\")\n",
        "\n",
        "    return total / max(n, 1)\n",
        "\n",
        "train_loss = run_epoch(train_loader, train=True)\n",
        "val_loss   = run_epoch(val_loader, train=False)\n",
        "print(f\"\\nDONE   train_loss={train_loss:.4f}  val_loss={val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "Pckvuv3avHEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"/content/blip_flickr8k_finetuned\"\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "processor.save_pretrained(SAVE_DIR)\n",
        "print(\"Saved to:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5BoQezwwyXW",
        "outputId": "8e17a572-3681-4dbb-f27a-31c2041843db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to: /content/blip_flickr8k_finetuned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_grouped = df[df[\"split\"]==\"test\"].groupby(\"image\")[\"caption\"].apply(list).reset_index()\n",
        "sample_rows = test_grouped.sample(10, random_state=42).reset_index(drop=True)\n",
        "\n",
        "results_samples = []\n",
        "\n",
        "for i, row in sample_rows.iterrows():\n",
        "    img_name = row[\"image\"]\n",
        "    refs = row[\"caption\"][:3]  # show 3 reference captions\n",
        "    img_path = os.path.join(images_dir, img_name)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_length=20, num_beams=5)\n",
        "    pred = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    results_samples.append({\n",
        "        \"image\": img_name,\n",
        "        \"prediction\": pred,\n",
        "        \"ref1\": refs[0],\n",
        "        \"ref2\": refs[1] if len(refs)>1 else \"\",\n",
        "        \"ref3\": refs[2] if len(refs)>2 else \"\"\n",
        "    })\n",
        "\n",
        "samples_df = pd.DataFrame(results_samples)\n",
        "samples_df\n"
      ],
      "metadata": {
        "id": "oR3wJg2pw4ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install nltk\n"
      ],
      "metadata": {
        "id": "y7F0yeVPxACn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "smooth = SmoothingFunction().method1\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on 200 test images for speed\n",
        "test_subset = test_grouped.sample(n=min(200, len(test_grouped)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for _, row in test_subset.iterrows():\n",
        "    img_name = row[\"image\"]\n",
        "    ref_caps = row[\"caption\"]\n",
        "    img_path = os.path.join(images_dir, img_name)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_length=20, num_beams=5)\n",
        "    pred = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # BLEU expects tokenized refs/preds\n",
        "    predictions.append(pred.split())\n",
        "    references.append([r.split() for r in ref_caps])\n",
        "\n",
        "bleu_score = corpus_bleu(references, predictions, smoothing_function=smooth)\n",
        "print(\"BLEU (200 test images):\", bleu_score)\n"
      ],
      "metadata": {
        "id": "-oRoUNMmxEiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss2 = run_epoch(train_loader, train=True)\n",
        "val_loss2   = run_epoch(val_loader, train=False)\n",
        "print(f\"IMPROVED  train_loss={train_loss2:.4f}  val_loss={val_loss2:.4f}\")\n"
      ],
      "metadata": {
        "id": "-cky0y-7xhIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "smooth = SmoothingFunction().method1\n",
        "model.eval()\n",
        "\n",
        "# Recreate grouped test set (if not already)\n",
        "test_grouped = df[df[\"split\"]==\"test\"].groupby(\"image\")[\"caption\"].apply(list).reset_index()\n",
        "\n",
        "# Evaluate on 200 test images for speed\n",
        "test_subset = test_grouped.sample(n=min(200, len(test_grouped)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for _, row in test_subset.iterrows():\n",
        "    img_name = row[\"image\"]\n",
        "    ref_caps = row[\"caption\"]\n",
        "    img_path = os.path.join(images_dir, img_name)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_length=20, num_beams=5)\n",
        "    pred = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred.split())\n",
        "    references.append([r.split() for r in ref_caps])\n",
        "\n",
        "bleu_score_2 = corpus_bleu(references, predictions, smoothing_function=smooth)\n",
        "print(\"BLEU (200 test images) AFTER 2 epochs:\", bleu_score_2)\n"
      ],
      "metadata": {
        "id": "rV88c-Iuz_58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_table = pd.DataFrame([\n",
        "    {\"Run\": \"Epoch 1 (baseline)\", \"Train Loss\": 2.3334, \"Val Loss\": 2.2349, \"BLEU@200\": 0.17805705554978052},\n",
        "    {\"Run\": \"Epoch 2 (improved)\", \"Train Loss\": 2.0635, \"Val Loss\": 2.2337, \"BLEU@200\": 0.1944406075573781},\n",
        "])\n",
        "\n",
        "results_table\n"
      ],
      "metadata": {
        "id": "hoHnx3s92CXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_csv_path = \"/content/drive/MyDrive/blip_results_table_epoch2.csv\"\n",
        "results_table.to_csv(results_csv_path, index=False)\n",
        "print(\" Saved:\", results_csv_path)\n"
      ],
      "metadata": {
        "id": "enDGBs_d2gnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_grouped = df[df[\"split\"]==\"test\"].groupby(\"image\")[\"caption\"].apply(list).reset_index()\n",
        "sample_rows = test_grouped.sample(10, random_state=7).reset_index(drop=True)\n",
        "\n",
        "qual_rows = []\n",
        "\n",
        "for _, row in sample_rows.iterrows():\n",
        "    img_name = row[\"image\"]\n",
        "    refs = row[\"caption\"][:3]\n",
        "    img_path = os.path.join(images_dir, img_name)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_length=20, num_beams=5)\n",
        "    pred = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    qual_rows.append({\n",
        "        \"image\": img_name,\n",
        "        \"prediction_epoch2\": pred,\n",
        "        \"ref1\": refs[0],\n",
        "        \"ref2\": refs[1] if len(refs)>1 else \"\",\n",
        "        \"ref3\": refs[2] if len(refs)>2 else \"\",\n",
        "    })\n",
        "\n",
        "qual_epoch2 = pd.DataFrame(qual_rows)\n",
        "qual_path = \"/content/drive/MyDrive/blip_qualitative_epoch2.csv\"\n",
        "qual_epoch2.to_csv(qual_path, index=False)\n",
        "print(\" Saved:\", qual_path)\n",
        "\n",
        "qual_epoch2.head(5)\n"
      ],
      "metadata": {
        "id": "lY0BX_MT2jfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss3 = run_epoch(train_loader, train=True)\n",
        "val_loss3   = run_epoch(val_loader, train=False)\n",
        "print(f\"EPOCH 3  train_loss={train_loss3:.4f}  val_loss={val_loss3:.4f}\")\n"
      ],
      "metadata": {
        "id": "8HGAxdcT25V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "smooth = SmoothingFunction().method1\n",
        "model.eval()\n",
        "\n",
        "test_grouped = df[df[\"split\"]==\"test\"].groupby(\"image\")[\"caption\"].apply(list).reset_index()\n",
        "test_subset = test_grouped.sample(n=min(200, len(test_grouped)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for _, row in test_subset.iterrows():\n",
        "    img_name = row[\"image\"]\n",
        "    ref_caps = row[\"caption\"]\n",
        "    img_path = os.path.join(images_dir, img_name)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_length=20, num_beams=5)\n",
        "    pred = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred.split())\n",
        "    references.append([r.split() for r in ref_caps])\n",
        "\n",
        "bleu_score_3 = corpus_bleu(references, predictions, smoothing_function=smooth)\n",
        "print(\"BLEU (200 test images) AFTER 3 epochs:\", bleu_score_3)\n"
      ],
      "metadata": {
        "id": "1v1wJZHd4ZOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_table_3 = pd.DataFrame([\n",
        "    {\"Run\": \"Epoch 1 (baseline)\", \"Train Loss\": 2.3334, \"Val Loss\": 2.2349, \"BLEU@200\": 0.17805705554978052},\n",
        "    {\"Run\": \"Epoch 2 (improved)\", \"Train Loss\": 2.0635, \"Val Loss\": 2.2337, \"BLEU@200\": 0.1944406075573781},\n",
        "    {\"Run\": \"Epoch 3 (final)\", \"Train Loss\": float(f\"{train_loss3:.4f}\"), \"Val Loss\": float(f\"{val_loss3:.4f}\"), \"BLEU@200\": float(bleu_score_3)},\n",
        "])\n",
        "\n",
        "results_table_3\n"
      ],
      "metadata": {
        "id": "BRlzFSZd41JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_csv_path = \"/content/drive/MyDrive/blip_results_table_epoch3.csv\"\n",
        "results_table_3.to_csv(results_csv_path, index=False)\n",
        "print(\" Saved:\", results_csv_path)\n"
      ],
      "metadata": {
        "id": "ZDxz9xNH5IQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FINAL_DIR = \"/content/drive/MyDrive/blip_flickr8k_finetuned_epoch3\"\n",
        "model.save_pretrained(FINAL_DIR)\n",
        "processor.save_pretrained(FINAL_DIR)\n",
        "print(\" Saved final model to:\", FINAL_DIR)\n"
      ],
      "metadata": {
        "id": "sua0ox8p5fGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END OF FILE"
      ],
      "metadata": {
        "id": "MPqwZD5WnsOd"
      }
    }
  ]
}